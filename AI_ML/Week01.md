# Fast-Tracking course of AI

The journey From `"What is AI?"` to `"How ChatGPT works"`.

<details>
  <summary>What is <b>Learning</b> ? </summary>

<br/>

What does it mean to learn? <br/>
**What is Learning?** <br/>
- Changing yourself based on experiences, basically adjustments
- Looking at things, touching it, Understanding it, experiencing it.

**Learning is a loop**.
- Try something -> See what happens(feedback) -> Adjust -> Repeat 

a basic scerio of Learning : Learning Cycle in childhood, Writing, Reading etc.

</details>

<details>
  <summary> What is <b>knowledge</b> ? </summary>

<br/>

What does it mean to know? <br/>
What is knowledge? <br/>
**Getting data and information about something.**

Two types of knowledge:
1. **Explicit Knowledge**
- Capital of India is Delhi
- Objective, It is either correct or Incorrect.

2. **Implicit Knowledge**
- How do you remember the face of your friends?
- Subjective, It is not fixed some people remember by nose, some are by eyes and so on.

</details>


<details>
  <summary> What is <b>Intelligence</b> ? </summary>

<br/>

What is Intelligence? <br/>
This is a great question is philosophy, Human Mankind History. 

**Intelligence is the ability to achieve goals in a wide range of situations.**
- Ability to perform/achieve in wide range of situations/activities.

Example : <br/>
**Calculator** does perform all the arithmetics, but It's not intelligent bcuz It does just only arithmetics. <br/>
**Human** does perform a wide ranges of catalogs(activities), we can also perform arithmetic, use Calculator, So Human is intellegent.


</details>

<details>
  <summary>What is <b>AI</b> ?</summary>

<br/>

What comes to mind when you hear "Artificial Intelligence"?

We human(Developers/Engineers/Scietists/Mathematicians) somehow turning intelligence to teach computers(machines) on wide range of tasks.
- Rule defined instructions to machines for a specific tasks, like perform arithmetic, turn on/off the light, speak what is feed etc.
- We wanted to make machine do general stuffs for General tasks not only specific tasks which is defined in the rules. Like understand
  the sarcasm, making jokes, answering basic general questions.

Out goal is make Computers(Machines) like Computer can understand, can have intelligence so that perform wide range of tasks 
instead of just specific defined rules or instructions based tasks.

</details>

<details>
  <summary>The <b>Goal of AI </b></summary>

<br/>

**Making machines do things(tasks) that would require intelligence if a human did them.**

Examples :
- Recognizing faces
- Understanding language
- Playing chess
- Driving a car

**How does we make this happen?** <br/>
In 1960-70s 1st thing came into people mind is, If they want to make machine intelligent then ***Write rules to make machines learn***
like different rules for different stuffs, so rules were designed , invented and introduced for different different
purposes.

</details>

<details>
  <summary> <b> Attempt #1: Write the Rules</b> </summary>

<br/>

> If the email contains 'free money' ->  **spam**

> If temperature > 100°F ->  **fever/cold**

> If chess piece can move to square AND square has enemy piece -> **capture**

This is called **"Expert Systems"** or **"Rule-Based AI"**


</details>

<details>
  <summary> <b>The Problem with Rules</b> : <code> How would you write rules for tasks that humans do intuitively?</code></summary>

<br/>

How would you write rules for tasks that humans do intuitively?

**Recognizing a Face**
> How do you define the exact distance between eyes or the curve of a nose for every possible angle and lighting?

**Understanding Sarcasm**
> "Oh, great." Does it mean something is good, or is it a complaint? Rules can't easily capture tone and context.

**Decoding Idioms**
> "It's raining cats and dogs." A rule-based system might look for falling animals instead of understanding the metaphor.

Problem with Rules is : It is impossible to make machines based on rules to do the stuffs like human intuitions.

But Writing Rules were not enough and this was not the ultimate goal. So, Research continued and a lot of stuffs happened.

Let's not make rules, Let's start with data, this is time when the ***Machine Learning Ideas emerged***.
- If we give machines(computer) a lot of data and let them to ***Statistical based learning*** based on data.
- Meaning the words appears, what it is followed by and what it follows based on that machines try to get a sense of what is happening?
- Shows, feed the machines 1000s of examples of different fields and expect machines to learn and do things from this data without
  explicitly using the rules/programming or anything.

</details>

<details>
  <summary> <b> Attempt #2: Let Machines Learn</b> i.e. <code>Machine Learning</code> </summary>

<br/>

Instead of writing rules... , Show the machine thousands of examples and let it figure out the patterns.

This is called **Machine Learning (ML)**

Basically what ML is, we do not  want to write rules, instructions, codes, programming explicitly, but we want Machines to internally 
write tools, codes other stuffs for itself based on data. This is the whole idea of Machine Learning. 

</details>

<details>
  <summary> <b> How Machine Learning Works (Intuitively)</b> ? </summary>

<br/>

1. Show the machine many examples
2. Machine makes a guess
3. Tell it if it was right or wrong
4. Machine adjusts itself slightly
5. Repeat millions of times

> **The Analogy : "Like a child learning by trial and error"**

How Machine Learning works? <br/>
We kind of make a bucket and put the input based on the data we trained them off.

<img width="350" height="250" alt="image" src="https://github.com/user-attachments/assets/29c3c1cf-a418-4f3b-9b5b-59d3e4a30932" />


</details>

<details>
  <summary> <b> AI vs ML</b> </summary>

<br/>

AI The Goal: Smart Machines
ML One Method: Learning from Data
Deep Learning

<img width="450" height="300" alt="image" src="https://github.com/user-attachments/assets/8a00708d-6305-472e-89f3-0d5d9d0c470c" />

Deep Learning is a subset of Machine Learning which deals which Neural Networks. <br/>
All the new models are based on something called ***Neural Network*** or ***Transformer***
- These are the smallest unit of this big system.
- It is like a **neuron**
- It is designed to mimic human brain where we have neurons which get fired when we listen, see, do an action based on that concept we
  have **Deep Learning** (Here, we explicitly relie on Neural Network)

> **Neuron**
> - A neuron is the fundamental unit of the nervous system, also known as a nerve cell
> - It is an electrically excitable cell responsible for transmitting information throughout the body via electrical and
chemical signals.
> - Neurons process and relay messages between different parts of the brain, spinal cord, and the rest of the body,
enabling functions such as movement, sensation, thought, and emotion.
> - The human brain contains approximately 100 billion neurons, each connected to thousands of others, forming vast networks
that underlie all cognitive and physiological functions.
> - Neurons are highly specialized and typically do not regenerate after damage, which contributes to the severity of brain
and spinal cord injuries. 

</details>

<details>
  <summary> <b> Early ML : Modest Successes</b> </summary>

<br/>

Early modest success of ML Model was : Google started using Statistical Model a lot in their systems, in their a lot of enterprises 
based solution like Email Spam filtering, recommendation systems.

Earlier phase, this was open field, People excited for that one day, we will solve all the agony(struggle) of human will, something better and practical. e.g. : A machine like ourself so that we don't have to do the ugly work, we can just sit and relax and make this machine better.

Still bottleneck with this system, couldn't have conversation with us. It is not exactly relying on **Rules** but **Statistical Data**. It works with lower accuracy  but It can't form its own sentences or articulate something for us. It only understand ***high ranking keywords*** based on Statistical data.


| What Worked                             | The Bottleneck                                  |
|-----------------------------------------|-------------------------------------------------|
| Spam filters got better                 | Couldn't have a conversation                    |
| Recommendation systems (Netflix,Amazon) | Couldn't understand a paragraph                 |
| Basic image recognition                 | Couldn't recognize objects as well as a toddler |

So, we came from nasty rules to learn something from statistical data and idea back then was to filed by field apply the formaula 
everywhere and at the end we have a unified machine which have something like **Weather Predictions**, **Email Classification**, 
**Movie Recommendations**, **Destination Planning** etc kind of Models which are trained on statistical data.

Classify Something but ***not with great accuracy*** But at least we have better than what we have before.


</details>


<details>
  <summary><b>Why Early ML Was Limited </b></summary>

<br/>

It was earlier internet era 1970-80 and data are not available online to feed the machines. So, Machines were so dumb. So they needed a lot of computer instructions.

`Bottleneck 01` : **Not enough data**
- Machine Learning requires massive amounts of examples to learn patterns effectively.
- Where do you get millions of labeled examples?
- The internet was in its infancy.

`Bottleneck 02` : **Not enough compute**
- Learning from data requires intense mathematical calculations.
- Computers were too slow for deep training.
- Training on millions of examples would take years.

But people were hopeful and raising funds for it, betting on something. But see today a lot of things changed. <br/>
Lacking a lot of things like Compute power(CPUs for processing), we only have data.

</details>

<details>
  <summary><b>AI Winters: When Hope Died (Twice)</b></summary>

<br/>

`1970s Winter` : **The First Collapse** <br/>
Researchers promised human-level AI in 20 years. When they failed to deliver, funding dried up and interest vanished.

`1980s–90s Winter` : **The Expert System Bust** <br/>
Expert systems were supposed to revolutionize business, but they were too brittle for the real world. Funding vanished again.

> Buzz of AI is dying field, The field became a joke. Saying you worked on 'AI' was career suicide.

A lot of funding pulled off from AI Startup companies which were trying to make statistical model better. No Data, No compute power for large datasets.

But after 2012, Internet blown off, Data content online spreading and freely available, Nvedia making GPUs for Gaming which were perfect for Math at very fast speed. So compute is also available. 

</details>


<details>
  <summary><b>Why AI Exploded (After 2012) </b></summary>

<br/>

`1. Data` : **The Internet** <br/>
Massive datasets of text, images, and video became available for the first time.

`2. Compute` : **GPUs** <br/>
Graphics cards, built for gaming, turned out to be perfect for the math ML needs.

`3. Algorithms` : **Deep Learning**  <br/>
Researchers finally cracked how to train much deeper and more complex neural networks.

> All three ingredients converged around 2012.

So from 1970-80s to till 2012 a lot of problems were solved. We have **Data**, we have **Compute** and we have **algorithms** which can make machine intelligent. So, We bet on it and next what happened was everything is infront of us.

</details>

## Now Let's Go Deep on One Problem

<details>
  <summary> Before Transformer, CNN - Convolutional Neural Networks</summary>
  
Before **transformer** also we have some successes not as as after transformer but still enough to lead us to transformer. <br/>
After **transformer**, Machine were started able to understand things contextually. <br/>
Before **transformer**, we had something known as **CNN**.

**CNN**
- Convolutional Neural Networks (CNNs) were the dominant architecture in computer vision for over a decade, serving as the foundation for most image recognition, classification, and detection tasks.
- It is a mathematical function basically help us convoluting a matrix. It was able to understand images. So, what happened was Image Field boomed.
- **Luis von Ahn** is the primary inventor of the CAPTCHA solution : we can easily pass an image of captcha and get the desired text out of it.

Images were working but again there were no context of it.<br/>
Like if we pass an image to a CNN, we will not get the description bcuz images do not have any description (context). <br/>
It was just a classification and that's why that is not the real intelligence.

- What it actually was, we gave millions of Images and categorized them in 10 classes/buckets. And ask model that your goal is to classify these images to these buckets.

People started to research more on languages and images to see if we can fit a mathematical formula of convulation to make much reliable and somehow pass the context to it. Later all the stuffs happened what we see.

"Attention Is All You Need" paper, published in 2017 by Ashish Vaswani and colleagues at Google Brain, introduced the Transformer architecture. That basically gives us Transformer. But that's not the whole thing. The paper only give the architecture of transformer but main part of whole architecture is something we call **Attention**.

The core idea of attention was given near back into different paper from **Bahdanau Attention** which laid the foundation for modern architectures like Transformers, which further refined attention mechanisms.  It is considered one of the most influential ideas in deep learning over the past decade. 

</details>

We'll use language understanding as our lens.

<details>
  <summary> <b> Why Language is Hard for Computers</b> ?</summary>

<br/>

> "I saw the man with the telescope."
> - Who had the telescope? Me or the man?

**Computers need precision, but human language is messy, ambiguous, and context-dependent.**

A single sentence can have multiple valid interpretations, making it one of the hardest problems in AI.

> "Fast-tracking the AI course" and "Fast-tracking the course of AI"

As we human we got so much confused by just channging order of one word, imagine how much machine would get confused bcuz they don't even know words they just understand numbers.

So, started working with languages bcuz we understand how hard it is to work for machines. The field is knows as **[NLP](https://www.ibm.com/think/topics/natural-language-processing)**.

**NLP = Natural Language Processing** <br/>
How intelligence work in Languages? <br/>
Basically It starts from How computers were, how languages were, first statistical model, anogram models. <br/>
We go throughly through 4 different aspects for it like How it ended up Transformer. <br/>

</details>


<details>
  <summary> <b>NLP Attempt #1: The Dictionary Approach </b> </summary>

<br/>

**Apple**
`/ˈap(ə)l/`
1. The round fruit of a tree of the rose family, which typically has thin red or green skin and crisp flesh.

`The Problem` : **A dictionary definition isn't enough for real-world language.**
- "Apple" is also a trillion-dollar company.
- "Apple" is a famous record label.
Here, Context changes everything.

</details>


<details>
  <summary> <b>NLP Attempt #2: Statistical Patterns </b> </summary>

<br/>

> If 'New' is followed by 'York' =>  City

This approach powered Google Translate for nearly a decade.

`The Problem` : **Pattern Matching ≠ Understanding**

The machine could predict the next word based on frequency, but it had no concept of what the words actually meant.

</details>

<details>
  <summary> <b>The Breakthrough: Words as Numbers </b> </summary>

<br/>

We understand that words are very hard for machines to understand. Machines literally do not know something which is known as word. Bcuz Machines works generally with different pointers and high precision numbers. So words literally make no sense for them.

But what if we can somehow transform these **words or different tokens** into some kind of numbers this was the first break through.

We can transform words to different types of vector, and vectors are just list of different numbers.

Now what will happen is Apple is completely different for APPLE as fruit and APPLE as a company or lable just based on the numbers.

How can we do that? <br/>
Different companies have different architecture and different learning algorithms like one company trained based on 2 char tokens or 1 char token or 1 word/token etc etc.based on that they have different vocabulary.

**"Apple"**  ->  **[0.92, -0.14, 0.05, ...]**

Computers don't understand **words**. They understand **numbers**. 

The challenge is: how do we turn a word into a list of numbers that captures its **meaning**?

### **Word Embeddings (The "Secret Sauce")**

Basically transforming any words(tokens) to it in vector form is known as **Word Embedding**

How do we turn a word like "Apple" into a number that captures its meaning?

Instead of one number, we give each word a list of numbers (a vector).

`The Vector Representation ` of Word : "Apple" is [0.9 - color(red) , -0.1 shape(square), 0.05, ...]

Each number represents a specific "dimension" of meaning.

That's why we transform any word into word embedding. Now what we have instead of us passing just a word which is non-existing to a machine, we something in form of numbers and those numbers have meaning that what the word is composed of. And now the list of words(vectors) can be pass down to a machine. So we have a mapping between a words and its vectors and this vectors can be now pass down to computer as these are numbers and computers(machines) works really well with the numbers.

<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/332543fe-709c-446b-993a-3f269b96201e" />


### **Dimensions of Meaning**

| Word | Royalty | Gender (M) | Edibility |
|------|---------|------------|-----------|
| King | 0.98    | 0.95       |  0.01     |
| Queen| 0.97    | 0.05       |  0.02     |
| Apple| 0.02    | 0.00       |  0.94     |

- "King" and "Queen" have similar numbers for royalty but vastly different values for gender.
- This is how machines represent concepts as data.


  We can see that these words make lot of sense as we make convert them in numbers (vectors). <br/>
  But what we can do with these words being tranformed into vectors(numbers)? what happen now we got the meaning of word.

  Assume there is a very big dimension(n-dimension of vector) space with difference of meaning, each dimesion contains different meaning.
  Let's look in a simplified diagram of it.
  - King and Queen are places closed to each other, closed to man and woman but apart from Apple and orange
  - Man and Woman are placed closed to each other
  - Apple and oranges are placed closed to each other 

### **Words as Positions in Space**
- If a word is a list of numbers, it's a point on a map. **Similar words are close together**, while different words are far apart.

<img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/f758a945-14f2-4e6f-a027-c80deb03cdc0" />


### **Visualizing Word Embeddings**
- Similar words are placed close together in a multi-dimensional space.
- Proximity indicates shared meaning.

<img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/abf037a2-e23c-4775-95d7-8230fb8eac27" />


This is how different word with different meaning and then these words transform to different vectors vectors has many dimension i.e. represented by numbers and each number has different meaning or direction like it can be gender, color, shape, size, it can be literally anything there are 100s of 1000s of dimension we can't even think of what kind of composition it can be of. But we can simplifies and understand it in 2D/3D that how it is happening.

### **The Magic of Embeddings - Word Math**
`King − Man + Woman = Queen` <br/>
what happen if we remove the gender Man and add the gender woman to king it will become Queen.
- king and queen are royalty, similarly man and woman are gender.
- What we are doing here is basic math add and subtract.

`Paris − France + Italy = Rome`  <br/>
Capital is paris if we have country france, if we replace country france to Italy the capital will become Rome
 
`Walking − Walk + Swim = Swimming`  <br/>
If we remove action verb walk and replace it with another action verb swim, it will be swimming

This is what math does, and ended up opening a lot of doors for AI and in the field of ML. With this simple calculation we can understand different meaning of words on which d Machine is not even trainned on. Just based on the high dimesional matrixes and graphs we ended up learning compleletely new dimensions on which machine is not trained of. Learn a lot from the data triained on. 

<img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/08daa373-c73e-42f8-8255-50296e1a30cb" />

### **Can Embeddings Solve Our Earlier Problems?**

Still we have one single problem, look at these 2 sentences :

"I went to the **bank** to deposit cash" <br/>
"I sat on the **bank** of the river"

like we have each word has a single vector attached to it but these vectors are not talking to each other. So "I" vector, "ewent" vector, "bank" vector they do not know about each other. 

The Problem: In basic word embeddings, each word has exactly ONE position in space.

The model can't distinguish between a financial institution and a riverbank because it doesn't look at the surrounding words.

So basically Embedding only works on words It does classification very accurate and well. But sentences it doesn't work. But when it comes to talking or sentences bcuz it doesn't know what came before and what will came after.

Even if know that Okay One vector knows I came after another vector and after me there is another vector then also the problem is still there bcuz It doesn't correlate the thing without us doing anything.

So, the ideas is to process the whole sequence. <br/>
if I have the whole sentence, I will process "Hi", "Hi My", "Hi My name" "Hi My name is", "Hi My name is Shiva" 

This was the first idea the research started on it and the area of sequence models emerged and It has like different kind of Stuffs like RNN, SM, GRUs and transformer which is the most advanced model. 

- Recurrent Neural network(RNN)
- Long Short Term Memory (LSTM)Copy link
- Gated Recurrent Units (GRU)
- Sutskever’s model (SM)
- TransformerCopy link

</details>

<details>
  <summary> <b>NLP Attempt #4: Sequence Models (RNNs)</b> </summary>

<br/>

Now we know that we can understand that we can know and undertstand meaning of a single word completely and some basic arithmetic operations like addition subtraction to it But what we can still not do is can't understand the complete sentence or form a sentence.
What is the basic thing we can do here is : what if we process the sentences word by word since we know the meaning of each word  and somehow make a system which keeps the memory from these previous words and add these to upcoming words.

Idea: Process the sentence one word at a time, building up understanding as you go.

> "I"  : `I vector fires up`<br/>
> "I went" : `Here instead of fire up 2 vectors separately, we somehow comprise 2 vectors into 1 and that vector is fired up` <br/>
> "I went to" : `vector comprises of I+went+to fire up` <br/>
> "I went to the" : `vector comprises of I+went+to+the fire up` <br/>
> "I went to the bank..."  : `vector comprises of I+went+to+the+bank fire up`<br/>

Based on this can we solve something, yes!. <br/>
As every word is getting added the meaning of it (which we derived from its vector i.e. numbers) is being added and what addes up at the end is ended up with a completely new vector which has a number. <br/>
So basically solve the problem that now we have different vectors for the word **bank** now.

This is how the origin of sequence models came in and we call then **Recurrent Neural Network**. Basically here, there is a concept of Recurrent here go, we're moving. This is an architecture which is based on Neural Network and We know that Neural Networks are the field which deals with Deep Learning, so we're learning about Deep Learning. 

So these RNN what they do is basically word by word process something and from each word they maintain some memory.
So here the context(meaning) of a word changes completely with upcoming words in a sentence.

The model maintains a "memory" of what it has read so far.


But as the sentences become longer, paragraph become longer and longer and story become longer. How much you can change the vector, how much you can remembers. This is an ever lasting problem of memory. You can't remember everything there would be a time when stuffs mess it up and your architecture and model blown off.

### The Problem: Forgetting**

`Long-Range Dependency Failure`
> "**The cat**, which was sitting on the mat that I bought from the store near the old church on the corner, **was** happy."

When the sequence model reach at "was", it forgetten what was the subject, bcuz problem is forgetting how many vectors it will remember by adding, the meaning will be completely disruppted when it will come to "was". This happens in all the big sentences for small It does work. 

Unlike Tranformers (chatGPT), RNNs Sequential model has to remember each word like cat, mat, this that everything.

By the time a sequence model reaches "was," it has often **forgotten** the subject at the beginning.

RNNs struggle with long sentences because they process information linearly and have limited memory capacity.

After 2012 and until 2017 we hd 4 things :
1. Word Embeddings : How to understand words
2. Sequence Models(RNNs) : How to understand sentence word by word (only smaller sentences but failed on longer bcuz memory forget the things due to sequential linear cause the data loss, at the end become so big that It forget what was the subject, verb)
3. Tons of Internet Data
4. Powerful GPUs

This Field was stuck for few years a lot of things was happening something came after RNNs i.e LSTM they are put a gate for not important words in a sentence like "which" "is/am/are" now now model had options what to remember and what to not. So each word has a gate like do I have to remember this word or not. 

So LSTM(Long Short Term Memory Models) which came after RNNs, only thing they improved on RNNs was they added a gate on each word and gave to choice either we have to add this word to memory or not like It will remeber Subject, Verb and Object and not remember which, that, etc etc.
So in this case the information the word which has to be retained stored in memory is reduced surprisingly bcuz half of the words are not needed to be remembered bcuz they were not making sense to work or process them. 
But we bring 10 times longer sentence same thing will happen to LSTM what happened to RNNs in a big sentence.

That's happened with LSTM and then GMU came

</details>

<details>
  <summary> <b>The Stage Is Set (around 2017) </b> </summary>

<br/>

`What we had` : **Word embeddings** | **Sequence models (RNNs)** | **Tons of internet data** | **Powerful GPUs**

`The Problem` : 

> **The Word-by-Word Bottleneck** <br/>
> Processing text sequentially was slow and models still "forgot" the beginning of long sentences.

We have only one thing to solve that we don't have to forget. For that what best thing we can do is be very selective bcuz in a sentence there are 2 or 3 words which makes full sense for it. So if we retain those 2 or 3 words that's very good thing, we use as less memory as we can and retain only important words which make sense. This is what we have to solve in sequence Models.

So what was happening in sequential models or RNNs or LSTM were that We were moving in linear fashion 1st word to last word and process each word by putting a gate to it whether retain it or not. This has 2 problems :
1. Slow speed bcuz until we process first important word we can't process the second important word.
2. Memory problem.

What if there was a better way?

</details>


<details>
  <summary> <b>The Transformer Idea</b> </summary>

<br/>

`The Old Way` : **Sequential** <br/>
Process words one by one, like reading a book from start to finish.

Do we do that as a human while reading a book? <br/>
No, We as a human we just go through a sentences with our eyes and retain the best knowledge we can like Character name, whether it's villain or Hero doing crime or saving from crime. we just came through 1st page, and undertstand the second page which the Sequential Model can't bcuz they will read slow and also not retained the things so long.

So what would be a new way? Can we make an architecture something like we process each word simulatenously independently and at the end correlate them to each other somehow and then will get the best output. So Sequence we're doing parallelly so fast and remeber 1 or 2 important words which make sense to get the meaning of the sentence

`The New Way` : **Simultaneous** <br/>
Look at every word in the sentence at the same time.

That secret is **Attention**

> **The Secret: Attention** <br/>
> Attention enable the model to "attends" the most relevant words, no matter how far apart they are.

1. Simulatenously/parallely process the words which increases the speed
2. Attention gives us most relevant words
These two things alone made that a breakthrough what happen was now we can do things so fast, read so much information we have a lot of data we can train on and with that addition of attention now we don't care about memory It can be as long as we want we have models which sometime might forget but this method is 1000s time better than RNNs or LSTM. So Model has now infinite memory bcuz they're retaining only most relevant words based on every question We ask.

We will see this Attention in detail when we see the Transformer Architecture.

</details>

<details>
  <summary> <b> Attention in Action </b> </summary>

<br/>

`Processing the word "it"` : The **animal** didn't cross the street because **it** was too tired.

Who was too tired? <br/>
As a human -> We say Animal, bcuz our brain process it <br/>
Machine -> ? : Similar to our brain what Transformer do here is process all words simulatenously(parallelly) and retain most relvant words which make sense to it. As it process the word it it highlight the most relevant word to "it" is animal. How does it do we will see it later. 

When the model looks at the word **"it"**, the attention mechanism tells it to pay the most attention to **"animal"**.

This is how the model "knows" what the pronoun refers to, building a context-aware representation of every word.

### **Same Structure, Different Attention**

`Scenario A` : The **animal** didn't cross the street because **it** was too **tired**.

`Scenario B` : The **animal** didn't cross the **street** because **it** was too **wide**.

By changing just one word (**tired** vs **wide**), the model's attention shifts, correctly identifying what "it" refers to in each context.

### **Why Attention Is Powerful**

`1. No Forgetting` : **More Memory** <br/>
Every word in a sentence "sees" every other word simultaneously, no matter how far apart they are.

We're only selecting most relevant words which are just selecting 1% things of LSTM and LSTM were selecting 50% thing of RNNs. So we can see how much big gap from earlier models we created in terms of memory and what this allows us to make a very mathematical mapping  to every word, how it is related to every different word and how the meaning changes with every different word.

`2. Speed` : **Parallel Processing** <br/>
Unlike RNNs that read word-by-word, Transformers process all words at once, making training incredibly fast.


`3. Context` : **Deep Understanding** <br/>
The model builds a mathematical map of how every word relates to every other word in the specific context.


If we look back to when it started where we have defined a set of rules if an object checks this rules then it is considered as intelligent. then after that we moved to statistical Model which was just doing classification no minute details or context then we moved to RNNs where word embedding, vectors mapping  and then LSTM where we added a gate to each words in RNNs and we reach to Transformer.
We can also understand images by same architecture. 

STORY TIME : FUNNY PHD STUDENTS <br/>
Around 2016-17, a lot of Students who were working on AI and ML field Applications. Anyone was not sure that Transformer will boom and disrupt everything and it is going to be the go to source of intelligence. Everyone was doing their thesis on completely different things like someone was doing 3D Modelling or RNNs etc etc.  <br/>
What happened is as soon as Transformer came up, it got a lot of attraction but people thought let's continue their own things but what happen in within 1-2 years the same Transformer architecture were applying to all these different fields and it completely disrrupted all the things what they developed over the years.  <br/>
A single model, a single architecture was responsible for that. What allows to do that **Attention**. Bcuz if you look at thing everything a sequence like if we talk about images every pixel that come after a pixel is a sequence.

If we pass a picture of beach and ask the model Can you describe the place? The model is smart enough to answer It will light up "beach" word. Just like human it would get glance at it and answer with the word "beach". how it is doing that? From the same context it undertstand the language (word or sentence).

Similarly 3D Modeliing(basicallt It's sequence of different stuffs), Music/Audio(sequence of stuffs), Videos(sequence), images (sequence), sentences(sequence), words(sequence).

If Our model i.e. Transformer Architecture is performing so well on the sequence words, can't it be perform well on the sequences of images or audios or videos. Yes It can.

First we discovered on Languages but Now Trasformer is applied to every field.
- For Images we have **Vision Transformer**, Defusion was eariler which was overtaked by **Defusion Transformer**
- For Videos we have **Audio Transformer**

 Transformer : An architecture which built entirely on the mechanism of Attention 

</details>

<details>
  <summary> <b>The Transformer</b> </summary>
<br/>

**An architecture built entirely on the mechanism of <ins>attention</ins>**.

`Trait 01` : **No recurrence. No convolution. Just attention.**

`Trait 02` : **The foundation of all modern Large Language Models (LLMs).**

`Trait 03` : **Enables massive scaling of data and compute.**

</details>

<details>
  <summary> <b>How ChatGPT Works (Simplified)</b>? : <b>Transformer + Internet Data + Prediction</b> </summary>

<br/>

`01. Architecture` : **The Engine** <br/>
A massive neural network built entirely on the Attention mechanism we just explored.

`02. Architecture` : **The Knowledge** <br/>
Trillions of words from books, articles, and the entire public internet.

`03. Architecture` : **The Task** <br/>
A simple goal: given a sequence of words, predict the most likely next word.

</details>

<details>
  <summary> <b>Why "Predict the Next Word" Creates Understanding</b> </summary>

<br/>

**Grammar & Syntax** <br/>
Predicting the next word requires the model to internalize language structure and rules.

**Factual Knowledge** <br/>
Correct predictions rely on learned factual relations between concepts.

**Logic & Reasoning** <br/>
Following a chain of reasoning lets the model anticipate the appropriate continuation.

### **How It Generates Text**

`Step 01` : **The**

`Step 02` : The **quick**

`Step 03` : The quick **brown**

`Step 04` : The quick brown **fox...**

Generation is an **iterative loop**. <br/>
Each predicted word is added back to the input, and the model predicts the next word based on the updated context.

### **Let's See It Again**

Revisiting the demo with a new perspective.

`Look For 01` : The iterative, word-by-word prediction process.

`Look For 02` : How attention builds deep context awareness.

`Look For 03` : How "reasoning" emerges from simple prediction.

### The Unexpected Discovery: Bigger = Smarter

`01. Data` : **More Tokens** <br/>
Scaling from millions to trillions of words of training data.

`02. Parameters` : **More Size** <br/>
Scaling from millions to hundreds of billions of internal connections.

`03. Compute` : **More Power** <br/>
Scaling from days to months of training on thousands of GPUs.

> **The Result: Emergent Capabilities. Reasoning, coding, and logic appear spontaneously as models get larger.**

</details>

<details>
  <summary> <b>Foundation Models</b> </summary>

<br/>

`The Old Paradigm` : **Task-Specific AI**
- Model A: Translation
- Model B: Summarization
- Model C: Sentiment Analysis

`The New Paradigm` : **General-Purpose AI**
- One massive model trained on everything, capable of performing any language task through prompting.

**The "Foundation" is the base knowledge. We no longer build tools from scratch; we build on top of these giants.**

</details>


<details>
  <summary> <b>Where We Are (2024-2025)</b> </summary>

<br/>

`Trend 01` : **Multimodality** <br/>
AI is no longer just text. It can see images, hear voices, and speak back in real-time.

`Trend 02` : **Reasoning** <br/>
New models are designed to "think" before they speak, solving complex math and logic problems.

`Trend 03` : **Agents** <br/>
The shift from chatbots to agents that can use tools, browse the web, and complete multi-step tasks.


</details>
